import numpy as np
import pandas as pd
import re
import nltk
import string
import seaborn as sns
import matplotlib.pyplot as plt

df.drop(columns=['date', 'variation'], inplace=True)
fig = plt.figure(figsize=(7, 4))
d = df['feedback'].value_counts()

x = d.keys().to_list()
y = d.values.tolist()
sns.barplot(x=x, y=y)

fig = plt.figure(figsize=(7, 4))
sns.histplot(data=df, x='rating', bins=5, kde=True, binrange=(0, 5))

review_text = df.drop(['rating','date','variation','feedback'],axis=1)
pd.set_option('display.max_colwidth', None)
review_text

review_text_lower = review_text.apply(lambda x: x.astype(str).str.lower())
df['review_lower']=review_text_lower
df['review_lower']

string.punctuation
#defining the function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#storing the puntuation free text
df['review_nopunc']= df['review_lower'].apply(lambda text: remove_punctuation(text))
df['review_nopunc']

def remove_emoji(string):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)

df['review_noemoji'] = df['review_nopunc'].apply(lambda x: remove_emoji(x))
df['review_noemoji']

EMOTICONS=[':-<',':->']
def remove_emoticons(text):
    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')
    return emoticon_pattern.sub(r'', text)

df['review_noemoticons'] = df['review_noemoji'].apply(lambda x: remove_emoticons(x))
df['review_noemoticons']

#defining function for tokenization
def tokenization(text):
    tokens = re.split('W+',text)
    return tokens
#applying function to the column
df['msg_tokenized']= df['review_noemoticons'].apply(lambda x: tokenization(x))
df['msg_tokenized']

from nltk.corpus import stopwords
nltk.download('stopwords')
", ".join(stopwords.words('english'))

STOPWORDS = set(stopwords.words('english'))
def remove_stopwords(text):
    """custom function to remove the stopwords"""
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])

df["text_wo_stop"] = df["msg_tokenized"].apply(lambda text: remove_stopwords(text))
df["text_wo_stop"]

from nltk.stem.porter import PorterStemmer

stemmer = PorterStemmer()
def stem_words(text):
    return " ".join([stemmer.stem(word) for word in text.split()])

df["text_stemmed"] = df["text_wo_stop"].apply(lambda text: stem_words(text))
df["text_stemmed"]

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
    return " ".join([lemmatizer.lemmatize(word) for word in text.split()])

nltk.download('omw-1.4')
df["text_lemmatized"] = df["text_wo_stop"].apply(lambda text: lemmatize_words(text))
df.head()

#bow
from sklearn.feature_extraction.text import CountVectorizer
bow = CountVectorizer()
data_bow = bow.fit_transform(df['verified_reviews'])
print("n_samples: %d, n_features: %d" % data_bow.shape)
print(bow.get_feature_names())
print(data_bow.toarray())

#tfidf
from sklearn.feature_extraction.text import TfidfVectorizer
tf_idf = TfidfVectorizer()
data_tf = tf_idf.fit_transform(df['verified_reviews'])
print("n_samples: %d, n_features: %d" % data_tf.shape)
print(tf_idf.get_feature_names())
print(data_tf.toarray())